{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be669ab2",
   "metadata": {},
   "source": [
    "## Building a RAG-enabled PDF Chat App with SuperDuperDB and Open Source LLMs from HuggingFace like DeciLM 7B\n",
    "\n",
    "This Notbook outlines the development of a chat application capable of interacting with PDF documents, leveraging the integration of SuperDuperDB with large language models (LLMs). The chosen LLM for this prototype is DeciLM 7B Instruct, but the framework can be readily adapted to other Open Source LLMs from HuggingFace.\n",
    "\n",
    "**Key Advantages:**\n",
    "\n",
    "* **Modular Architecture:** SuperDuperDB seamlessly integrates with specialized Python libraries like PyMuPDF and Pandas, enabling efficient handling of distinct tasks like PDF parsing and text chunking. This modular approach offers superior performance compared to all-in-one solutions.\n",
    "* **Open Source Flexibility:** Utilizing Open Source LLMs from HuggingFace empowers developers with greater customization and cost-effectiveness compared to proprietary API offerings.\n",
    "* **Enhanced Scalability:** SuperDuperDB's vector indexing capabilities facilitate efficient retrieval of relevant information from large document collections, enabling the chat app to scale effectively.\n",
    "\n",
    "**Core Workflow:**\n",
    "\n",
    "1. **PDF Parsing and Chunking:** PyMuPDF and Pandas are utilized to extract text from the PDF document and segment it into manageable chunks suitable for LLM processing.\n",
    "2. **Vector Indexing and Storage:** SuperDuperDB creates a vector representation of each text chunk and stores it efficiently for retrieval.\n",
    "3. **LLM-powered Chat Interaction:** User queries are formulated as prompts, and SuperDuperDB retrieves relevant chunks from the indexed data. The chosen LLM processes these chunks and generates responses, enabling a conversational interaction with the PDF content.\n",
    "\n",
    "**Benefits and Applications:**\n",
    "\n",
    "This approach offers a robust framework for building chat applications capable of interacting with textual data stored in PDF format. Potential applications include:\n",
    "\n",
    "* **Information Retrieval:** Chatbots can answer user queries directly from within PDFs, improving document accessibility and knowledge extraction.\n",
    "* **Data Analysis:** LLMs can analyze large document collections through the chat interface, providing insights and summarizations.\n",
    "* **Educational Tools:** Interactive learning experiences can be built by enabling students to ask questions and receive answers directly from relevant PDFs.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "This framework demonstrates the effectiveness of combining SuperDuperDB with Open Source LLMs to create powerful chat applications for interacting with textual data. By leveraging the modularity and flexibility of this approach, developers can build scalable and customizable solutions for diverse applications across various industries.\n",
    "\n",
    "Let's begin üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828a2422",
   "metadata": {},
   "source": [
    "Install the libraries ü´°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90cb1a7-ba54-4723-8d88-5a364e851fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub\n",
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes\n",
    "!pip install ninja\n",
    "!pip install flash-attn\n",
    "!pip install sentence_transformers\n",
    "!pip install pymupdf\n",
    "!pip install superduperdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e39ce",
   "metadata": {},
   "source": [
    "# Step 1: PDF Parsing and Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d822f6fe",
   "metadata": {},
   "source": [
    "## Let's process a PDF file first in the most pythonic way possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef54e6",
   "metadata": {},
   "source": [
    "Download the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce27b3-f470-4aaf-aef4-1b95cdee800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "wget -O state_of_ai_2023.zip https://github.com/harpreetsahota204/langchain-zoomcamp/raw/main/State%20of%20AI%20Report%202023%20-%20ONLINE.pdf.zip\n",
    "unzip state_of_ai_2023.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a34c6b",
   "metadata": {},
   "source": [
    "## Supported Documents: PDF XPS EPUB MOBI FB2 CBZ\n",
    "Let's process a PDF file first in the most pythonic way possible. Use PyMuPDF library to parse any kind of documents in the most efficient way possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91985aea-25d1-40fe-b551-32af44110e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF content saved to output.json\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "\n",
    "def chunk_pdf_to_json(pdf_path, chunk_size=500):\n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    # Create a list to store text chunks\n",
    "    text_chunks = []\n",
    "\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text()\n",
    "\n",
    "        # Chunk the text into segments\n",
    "        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "        # Append each chunk to the list\n",
    "        text_chunks.extend(chunks)\n",
    "\n",
    "    # Close the PDF document\n",
    "    doc.close()\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "def save_to_json(output_file, text_chunks):\n",
    "    with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(text_chunks, json_file, ensure_ascii=False)\n",
    "\n",
    "def main():\n",
    "    # Specify the PDF file path\n",
    "    pdf_path = \"State of AI Report 2023 - ONLINE.pdf\"\n",
    "\n",
    "\n",
    "    # Specify the output JSON file path\n",
    "    output_file = \"output.json\"\n",
    "\n",
    "    # Chunk the PDF into text segments\n",
    "    text_chunks = chunk_pdf_to_json(pdf_path)\n",
    "\n",
    "    # Save the text segments to a JSON file\n",
    "    save_to_json(output_file, text_chunks)\n",
    "\n",
    "    print(f\"PDF content saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def23450",
   "metadata": {},
   "source": [
    "Now the good ol' pandas. Pandas is unbeatable for fast data processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad95f84f-3655-44b4-8ef9-b4e768128501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>State of AI Report\\nOctober 12, 2023\\nNathan B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>About the authors\\n Introduction | Research | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hD from Cambridge in cancer research. \\nNathan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>State of AI Report 2023 team\\n#stateofai | 3\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ssociate \\nDirector \\nat \\nMilltown \\nPartners...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>About the authors\\n Introduction | Research | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>PhD from Cambridge in cancer research. \\nNath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>State of AI Report 2023 team\\n#stateofai | 162...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>\\nAssociate \\nDirector \\nat \\nMilltown \\nPartn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>State of AI Report\\nOctober 12, 2023\\nNathan B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>384 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0    State of AI Report\\nOctober 12, 2023\\nNathan B...\n",
       "1    About the authors\\n Introduction | Research | ...\n",
       "2    hD from Cambridge in cancer research. \\nNathan...\n",
       "3    State of AI Report 2023 team\\n#stateofai | 3\\n...\n",
       "4    ssociate \\nDirector \\nat \\nMilltown \\nPartners...\n",
       "..                                                 ...\n",
       "379  About the authors\\n Introduction | Research | ...\n",
       "380   PhD from Cambridge in cancer research. \\nNath...\n",
       "381  State of AI Report 2023 team\\n#stateofai | 162...\n",
       "382  \\nAssociate \\nDirector \\nat \\nMilltown \\nPartn...\n",
       "383  State of AI Report\\nOctober 12, 2023\\nNathan B...\n",
       "\n",
       "[384 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = \"output.json\"\n",
    "\n",
    "# Read the JSON file into a Pandas DataFrame\n",
    "with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df\n",
    "\n",
    "# Now 'df' is a Pandas DataFrame containing the data from the JSON file\n",
    "# You can perform various operations on the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce0f529",
   "metadata": {},
   "source": [
    "To further process the PDF file and eliminate unnecessary content between texts, you can consider using text extraction techniques or regular expressions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e2fbcb-29b5-4efb-8c2a-346082dd84f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>State of AI Report October 12 2023 Nathan Bena...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>About the authors Introduction  Research  Indu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hD from Cambridge in cancer research Nathan Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>State of AI Report 2023 team stateofai  3 Intr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ssociate Director at Milltown Partners where h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>About the authors Introduction  Research  Indu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>PhD from Cambridge in cancer research Nathan B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>State of AI Report 2023 team stateofai  162 In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>Associate Director at Milltown Partners where ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>State of AI Report October 12 2023 Nathan Bena...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>384 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0    State of AI Report October 12 2023 Nathan Bena...\n",
       "1    About the authors Introduction  Research  Indu...\n",
       "2    hD from Cambridge in cancer research Nathan Be...\n",
       "3    State of AI Report 2023 team stateofai  3 Intr...\n",
       "4    ssociate Director at Milltown Partners where h...\n",
       "..                                                 ...\n",
       "379  About the authors Introduction  Research  Indu...\n",
       "380  PhD from Cambridge in cancer research Nathan B...\n",
       "381  State of AI Report 2023 team stateofai  162 In...\n",
       "382  Associate Director at Milltown Partners where ...\n",
       "383  State of AI Report October 12 2023 Nathan Bena...\n",
       "\n",
       "[384 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# Remove extra whitespaces using regex\n",
    "df[0] = df[0].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "df[0] = df[0].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3349c0",
   "metadata": {},
   "source": [
    "Now change the column name to `text_chunk` and create a `id` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "140386b8-162e-4399-b2da-c299633429e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                         text_chunk\n",
      "0      0  State of AI Report October 12 2023 Nathan Bena...\n",
      "1      1  About the authors Introduction  Research  Indu...\n",
      "2      2  hD from Cambridge in cancer research Nathan Be...\n",
      "3      3  State of AI Report 2023 team stateofai  3 Intr...\n",
      "4      4  ssociate Director at Milltown Partners where h...\n",
      "..   ...                                                ...\n",
      "379  379  About the authors Introduction  Research  Indu...\n",
      "380  380  PhD from Cambridge in cancer research Nathan B...\n",
      "381  381  State of AI Report 2023 team stateofai  162 In...\n",
      "382  382  Associate Director at Milltown Partners where ...\n",
      "383  383  State of AI Report October 12 2023 Nathan Bena...\n",
      "\n",
      "[384 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Reset the index and convert it to a column named 'id'\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 'id', 0: 'text_chunk'}, inplace=True)\n",
    "\n",
    "# Display the DataFrame with 'id' column\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888edb2d",
   "metadata": {},
   "source": [
    "# Step 2: SuperDuperDB for indexing vectors, searching it and persistency \n",
    "\n",
    "SuperDuperDB actually wears many hats, this hat is one of them. You can even do model training and other stuffs as well. But let's use only one power of SuperDuperDB today. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3852d4a1",
   "metadata": {},
   "source": [
    "## Now, SuperDuperDB enters in the room\n",
    "\n",
    "Now, introducing SuperDuperDB into the mix! You have the option to store the data in MongoDB or any other SQL database. Here, we demonstrate with Sqlite and DuckDB, offering the advantage of saving the database locally for future use on your filesystem.\n",
    "\n",
    "But why a Database is needed for Your PDF Chat App?\n",
    "\n",
    "You're absolutely right! While the chat application can function without a database for a limited number of PDFs, a database unlocks its true potential and offers many benefits. While a database-less approach might be feasible for small-scale use cases, integrating a database into your PDF chat app unlocks a world of benefits, enhancing efficiency, user experience, data security, and analytical capabilities. It's the key to building a truly robust and scalable solution for interacting with textual data in PDF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "baed586a-b4d9-4e55-9564-70b210a4ab78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2023-Dec-15 13:21:22.73\u001b[0m| \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mip-172-31-29-75\u001b[0m| \u001b[36ma7d1f8ff-42be-476b-9adc-74c4b826ac32\u001b[0m| \u001b[36msuperduperdb.base.build\u001b[0m:\u001b[36m50  \u001b[0m | \u001b[34m\u001b[1mParsing data connection URI:sqlite://book.db\u001b[0m\n",
      "\u001b[32m 2023-Dec-15 13:21:22.73\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mip-172-31-29-75\u001b[0m| \u001b[36ma7d1f8ff-42be-476b-9adc-74c4b826ac32\u001b[0m| \u001b[36msuperduperdb.base.build\u001b[0m:\u001b[36m137 \u001b[0m | \u001b[1mData Client is ready. <ibis.backends.sqlite.Backend object at 0x7f1f4c33cb80>\u001b[0m\n",
      "\u001b[32m 2023-Dec-15 13:21:22.74\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mip-172-31-29-75\u001b[0m| \u001b[36ma7d1f8ff-42be-476b-9adc-74c4b826ac32\u001b[0m| \u001b[36msuperduperdb.base.datalayer\u001b[0m:\u001b[36m79  \u001b[0m | \u001b[1mBuilding Data Layer\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from superduperdb import superduper\n",
    "from superduperdb.backends.mongodb import Collection\n",
    "import os\n",
    "\n",
    "# SuperDuperDB, now handles your MongoDB database\n",
    "# It just super dupers your database\n",
    "db = superduper(\"sqlite://book.db\") # For SQLITE \n",
    "# db = superduper('duckdb://test.ddb') # For DuckDB\n",
    "# (You can save it later after saving the pdf from your filesystem. But we recommend serious persistent database!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814a37e",
   "metadata": {},
   "source": [
    "Now create a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5de70622-e96d-4ad0-bb66-f40510fb816b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], Table())"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from superduperdb.backends.ibis.query import Table\n",
    "from superduperdb.backends.ibis.field_types import dtype\n",
    "from superduperdb import Schema\n",
    "\n",
    "# Define the 'captions' table\n",
    "book = Table(\n",
    "    'book',\n",
    "    primary_id='id',\n",
    "    schema=Schema(\n",
    "        'book-schema',\n",
    "        fields={'id': dtype(str), 'text_chunk': dtype(str)},\n",
    "    )\n",
    ")\n",
    "\n",
    "db.add(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f860db71",
   "metadata": {},
   "source": [
    "# Persistency is key\n",
    "\n",
    "Insert the book into the database. This is a one-time task, and you can continue adding as many books as needed. Let's begin with the previous book. It gives you persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3c9f55ba-170c-4574-8e26-af22771f7166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2023-Dec-15 13:21:31.43\u001b[0m| \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mip-172-31-29-75\u001b[0m| \u001b[36ma7d1f8ff-42be-476b-9adc-74c4b826ac32\u001b[0m| \u001b[36msuperduperdb.base.datalayer\u001b[0m:\u001b[36m716 \u001b[0m | \u001b[34m\u001b[1mBuilding task workflow graph. Query:<superduperdb.backends.ibis.query.IbisQueryTable[\n",
      "    \u001b[92m\u001b[1mbook\u001b[0m}\n",
      "] object at 0x7f1f240d1480>\u001b[0m\n",
      "\u001b[32m 2023-Dec-15 13:21:31.44\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mip-172-31-29-75\u001b[0m| \u001b[36ma7d1f8ff-42be-476b-9adc-74c4b826ac32\u001b[0m| \u001b[36msuperduperdb.backends.local.compute\u001b[0m:\u001b[36m32  \u001b[0m | \u001b[1mSubmitting job. function:<function callable_job at 0x7f201a217f40>\u001b[0m\n",
      "\u001b[32m 2023-Dec-15 13:21:31.44\u001b[0m| \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mip-172-31-29-75\u001b[0m| \u001b[36ma7d1f8ff-42be-476b-9adc-74c4b826ac32\u001b[0m| \u001b[36msuperduperdb.misc.download\u001b[0m:\u001b[36m337 \u001b[0m | \u001b[34m\u001b[1m{'cls': 'IbisQueryTable', 'dict': {'identifier': 'book', 'primary_id': 'id'}, 'module': 'superduperdb.backends.ibis.query'}\u001b[0m\n",
      "\u001b[32m 2023-Dec-15 13:21:31.44\u001b[0m| \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mip-172-31-29-75\u001b[0m| \u001b[36ma7d1f8ff-42be-476b-9adc-74c4b826ac32\u001b[0m| \u001b[36msuperduperdb.misc.download\u001b[0m:\u001b[36m338 \u001b[0m | \u001b[34m\u001b[1m[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383]\u001b[0m\n",
      "\u001b[32m 2023-Dec-15 13:21:31.55\u001b[0m| \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mip-172-31-29-75\u001b[0m| \u001b[36ma7d1f8ff-42be-476b-9adc-74c4b826ac32\u001b[0m| \u001b[36msuperduperdb.backends.local.compute\u001b[0m:\u001b[36m38  \u001b[0m | \u001b[32m\u001b[1mJob submitted.  function:<function callable_job at 0x7f201a217f40> future:153add25-742e-479a-80c0-20336275b232\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Insert data from the 'images_df' DataFrame into the 'images' table\n",
    "_ = db.execute(book.insert(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765be680",
   "metadata": {},
   "source": [
    "Now you use `all-MiniLM-L6-v2` to create embedding for the `text_chunk` in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "39e4c568-5d15-47f6-99c1-394aff5c759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-15 13:21:37] sentence_transformers.SentenceTransformer INFO Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "[2023-12-15 13:21:37] sentence_transformers.SentenceTransformer INFO Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "from superduperdb import Model\n",
    "import sentence_transformers\n",
    "from superduperdb.ext.numpy import array\n",
    "\n",
    "# Create a SuperDuperDB Model using Sentence Transformers\n",
    "superduperdb_model = Model(\n",
    "    identifier='all-MiniLM-L6-v2',\n",
    "    object=sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2'),\n",
    "    encoder=array('float32', shape=(384,)),\n",
    "    predict_method='encode',\n",
    "    batch_predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cecb737",
   "metadata": {},
   "source": [
    "Now create a VectorIndex called `book-index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d6706287-b72c-484c-800b-c31fafe13920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2023-Dec-15 13:21:41.01\u001b[0m| \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mip-172-31-29-75\u001b[0m| \u001b[36ma7d1f8ff-42be-476b-9adc-74c4b826ac32\u001b[0m| \u001b[36msuperduperdb.base.datalayer\u001b[0m:\u001b[36m873 \u001b[0m | \u001b[34m\u001b[1mencoder/numpy.float32[384]/1 already exists - doing nothing\u001b[0m\n",
      "\u001b[32m 2023-Dec-15 13:21:41.03\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mip-172-31-29-75\u001b[0m| \u001b[36ma7d1f8ff-42be-476b-9adc-74c4b826ac32\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m221 \u001b[0m | \u001b[1mAdding model all-MiniLM-L6-v2 to db\u001b[0m\n",
      "\u001b[32m 2023-Dec-15 13:21:41.03\u001b[0m| \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mip-172-31-29-75\u001b[0m| \u001b[36ma7d1f8ff-42be-476b-9adc-74c4b826ac32\u001b[0m| \u001b[36msuperduperdb.base.datalayer\u001b[0m:\u001b[36m873 \u001b[0m | \u001b[34m\u001b[1mmodel/all-MiniLM-L6-v2/1 already exists - doing nothing\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 768/768 [00:00<00:00, 184217.40it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:00<00:00, 74.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2023-Dec-15 13:21:41.66\u001b[0m| \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mip-172-31-29-75\u001b[0m| \u001b[36ma7d1f8ff-42be-476b-9adc-74c4b826ac32\u001b[0m| \u001b[36msuperduperdb.base.datalayer\u001b[0m:\u001b[36m873 \u001b[0m | \u001b[34m\u001b[1mmodel/all-MiniLM-L6-v2/1 already exists - doing nothing\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([None],\n",
       " VectorIndex(identifier='book-index', indexing_listener=Listener(key='text_chunk', model=Model(identifier='all-MiniLM-L6-v2', object=<Artifact artifact=SentenceTransformer(\n",
       "   (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "   (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "   (2): Normalize()\n",
       " ) serializer=dill>, flatten=False, output_schema=None, encoder=Encoder(identifier='numpy.float32[384]', decoder=<Artifact artifact=<superduperdb.ext.numpy.encoder.DecodeArray object at 0x7f1f1665f1f0> serializer=dill>, encoder=<Artifact artifact=<superduperdb.ext.numpy.encoder.EncodeArray object at 0x7f1f4c33f340> serializer=dill>, shape=(384,), load_hybrid=True, version=1), preprocess=None, postprocess=None, collate_fn=None, metrics=(), predict_method='encode', model_to_device_method=None, batch_predict=True, takes_context=False, train_X=None, train_y=None, training_select=None, metric_values={}, training_configuration=None, model_update_kwargs={}, serializer='dill', device='cpu', preferred_devices=('cuda', 'mps', 'cpu'), validation_sets=None, version=1), select=Table(), active=True, identifier='all-MiniLM-L6-v2/text_chunk', predict_kwargs={}, version=2), compatible_listener=Listener(key='text_chunk', model=Model(identifier='all-MiniLM-L6-v2', object=<Artifact artifact=SentenceTransformer(\n",
       "   (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "   (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "   (2): Normalize()\n",
       " ) serializer=dill>, flatten=False, output_schema=None, encoder=Encoder(identifier='numpy.float32[384]', decoder=<Artifact artifact=<superduperdb.ext.numpy.encoder.DecodeArray object at 0x7f1f1665f1f0> serializer=dill>, encoder=<Artifact artifact=<superduperdb.ext.numpy.encoder.EncodeArray object at 0x7f1f4c33f340> serializer=dill>, shape=(384,), load_hybrid=True, version=1), preprocess=None, postprocess=None, collate_fn=None, metrics=(), predict_method='encode', model_to_device_method=None, batch_predict=True, takes_context=False, train_X=None, train_y=None, training_select=None, metric_values={}, training_configuration=None, model_update_kwargs={}, serializer='dill', device='cpu', preferred_devices=('cuda', 'mps', 'cpu'), validation_sets=None, version=1), select=None, active=False, identifier='all-MiniLM-L6-v2/text_chunk', predict_kwargs={}, version=3), measure=<VectorIndexMeasureType.cosine: 'cosine'>, version=1, metric_values={}))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from superduperdb import VectorIndex, Listener\n",
    "\n",
    "# Add a VectorIndex\n",
    "db.add(\n",
    "    VectorIndex(\n",
    "        'book-index',\n",
    "        indexing_listener=Listener(\n",
    "            model=superduperdb_model,\n",
    "            key='text_chunk',\n",
    "            select=book, # Table Name\n",
    "        ),\n",
    "        compatible_listener=Listener(\n",
    "            model=superduperdb_model,\n",
    "            key='text_chunk',\n",
    "            active=False,\n",
    "            select=None,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "22ecf2d1-566a-4f2f-82b7-7650466fb85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book-index']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.show('vector_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c989bda4",
   "metadata": {},
   "source": [
    "Now test drive it. Let's do a vector search on the books. Here we searched for `What is new about FlashAttention?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4da3e2e3-d617-4714-8db4-ed2960ddea92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 181.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2023-Dec-15 13:21:53.52\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mip-172-31-29-75\u001b[0m| \u001b[36ma7d1f8ff-42be-476b-9adc-74c4b826ac32\u001b[0m| \u001b[36msuperduperdb.base.datalayer\u001b[0m:\u001b[36m124 \u001b[0m | \u001b[1mloading of vectors of vector-index: 'book-index'\u001b[0m\n",
      "\u001b[32m 2023-Dec-15 13:21:53.52\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mip-172-31-29-75\u001b[0m| \u001b[36ma7d1f8ff-42be-476b-9adc-74c4b826ac32\u001b[0m| \u001b[36msuperduperdb.base.datalayer\u001b[0m:\u001b[36m164 \u001b[0m | \u001b[1m<superduperdb.backends.ibis.query.IbisCompoundSelect[\n",
      "    \u001b[92m\u001b[1mbook.join(_outputs/all-MiniLM-L6-v2/1.relabel({'output': \"'_outputs.text_chunk.all-MiniLM-L6-v2.1'\"}), _outputs/all-MiniLM-L6-v2/1.relabel({'output': \"'_outputs.text_chunk.all-MiniLM-L6-v2.1'\"}).input_id == book.id).filter(_outputs/all-MiniLM-L6-v2/1.relabel({'output': \"'_outputs.text_chunk.all-MiniLM-L6-v2.1'\"}).key == 'text_chunk')\u001b[0m}\n",
      "] object at 0x7f1ffd19a0b0>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vectors into vector-table...: 0it [00:00, ?it/s]/home/ubuntu/project/superduperdb/env/lib/python3.10/site-packages/superduperdb/backends/ibis/query.py:818: FutureWarning: `Table.relabel` is deprecated as of v7.0; use `Table.rename` instead (if passing a mapping, note the meaning of keys and values are swapped in Table.rename).\n",
      "  return getattr(parent, self.name)(*args, **kwargs), tables\n",
      "Loading vectors into vector-table...: 1536it [00:00, 9510.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2023-Dec-15 13:21:53.58\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mip-172-31-29-75\u001b[0m| \u001b[36ma7d1f8ff-42be-476b-9adc-74c4b826ac32\u001b[0m| \u001b[36msuperduperdb.backends.ibis.query\u001b[0m:\u001b[36m235 \u001b[0m | \u001b[33m\u001b[1mDisambiguation not yet supported of _fold_right: TODO!\u001b[0m\n",
      "\u001b[32m 2023-Dec-15 13:21:53.69\u001b[0m| \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mip-172-31-29-75\u001b[0m| \u001b[36ma7d1f8ff-42be-476b-9adc-74c4b826ac32\u001b[0m| \u001b[36msuperduperdb.vector_search.in_memory\u001b[0m:\u001b[36m67  \u001b[0m | \u001b[34m\u001b[1m[ 0.1279827   0.2199108   0.1840778   0.13627723 -0.01266751  0.15201518\n",
      "  0.2332509   0.20212926  0.13149157  0.18559319  0.20774594  0.09110226\n",
      "  0.21026263  0.17289744  0.13998233  0.24578577  0.08389924  0.15822439\n",
      "  0.18548933  0.15634294  0.1690236   0.15244472  0.08105554  0.14042866\n",
      "  0.19324496  0.23143569  0.12598097  0.11050446  0.07055023  0.20252329\n",
      "  0.09383817  0.06189565  0.17707013  0.15875745  0.12750573  0.13243927\n",
      "  0.13937046  0.12853792  0.10664025  0.14330223  0.2526616   0.06014976\n",
      "  0.03820092  0.17658284  0.1255298   0.19222465  0.16840631  0.1735734\n",
      "  0.33879203  0.18535872  0.21007608  0.17317763  0.5015377   0.20856729\n",
      "  0.24947053  0.18583965  0.17176494  0.21034792  0.1788768   0.29105696\n",
      "  0.03449035  0.13759665  0.14320633  0.25413412  0.20329522  0.19570929\n",
      "  0.26109168  0.26449883  0.1970427   0.08872323  0.06751122  0.21061419\n",
      "  0.17135757  0.12533459  0.19255984  0.13710272  0.1868734   0.11894359\n",
      "  0.19813558  0.19816127  0.18613985  0.11867005  0.03356502  0.12923262\n",
      "  0.11100307  0.16674799  0.19417743  0.16259581  0.1798535   0.21094626\n",
      "  0.13269512  0.25718385  0.23717175  0.14113279  0.08628902  0.21459025\n",
      "  0.22782259  0.14124404  0.14617585  0.04461602  0.20417592  0.16576378\n",
      "  0.33230758  0.16729778  0.22126332  0.15320072  0.12853393  0.25935894\n",
      "  0.16931435  0.19903149  0.19640684  0.2194795   0.23779869  0.11125773\n",
      "  0.19255099  0.2447793   0.33343485  0.15364622  0.10957412  0.2695789\n",
      "  0.23548093  0.17047337  0.20017412  0.1095121   0.08709718  0.21397458\n",
      "  0.14562386  0.21565713  0.2172395   0.10204495  0.24762976  0.36382467\n",
      "  0.0765882   0.12744781  0.25127763  0.19982368  0.23152621  0.07846367\n",
      "  0.11233099  0.17543551  0.28067     0.16051069  0.23752514  0.2482854\n",
      "  0.04884288  0.13320085  0.20817113  0.21449354  0.19008791  0.11777627\n",
      "  0.17304046  0.19439307  0.03364909  0.10705129  0.13710272  0.26377553\n",
      "  0.06483617  0.08840667  0.16132194  0.22060284  0.07873674  0.09377681\n",
      "  0.08041567  0.12616937  0.21896872  0.21919407  0.09111634  0.3106687\n",
      "  0.15973902  0.18762624  0.22051579  0.20489895  0.34343058  0.21158804\n",
      "  0.22622195  0.18036231  0.17547375  0.1438627   0.11991292  0.10497908\n",
      "  0.12529434  0.17510574  0.01404753  0.21231982  0.20201856  0.1463367\n",
      "  0.10137304  0.22185245  0.0985782   0.09095557  0.21393925  0.17820448\n",
      "  0.13299453  0.18675792  0.03111652  0.23426305  0.166605    0.17975776\n",
      "  0.20938897  0.14424452  0.12305957  0.11933757  0.21835734  0.30039436\n",
      "  0.17197867  0.09353318  0.08902033  0.13370402  0.2630027   0.23064536\n",
      " -0.05417959  0.19975162  0.18309891  0.26695415  0.22799084  0.22979414\n",
      "  0.18494365  0.15412539  0.1973449  -0.00651165  0.14912672  0.15401267\n",
      "  0.15409988  0.14178462  0.15379284  0.15218554  0.12287501  0.20060167\n",
      "  0.23231134  0.18089011  0.19151574  0.23407127  0.1605487   0.27140778\n",
      "  0.04091039  0.2502346   0.1298889   0.22589222  0.23733512  0.06791121\n",
      "  0.13972975  0.22919294  0.02752168  0.200091    0.17682338 -0.00166868\n",
      "  0.14251223  0.19022319  0.1641039   0.20840856  0.2048018   0.19461295\n",
      "  0.18228003  0.2566641   0.15173158  0.19643933  0.19390887  0.08284059\n",
      "  0.16714358  0.12914374  0.25559047  0.21087517  0.14824     0.24171652\n",
      "  0.12556863  0.15167874  0.1704339   0.15597045  0.15331054  0.1854778\n",
      "  0.243034    0.20170961  0.26882356  0.20563722  0.28887504  0.25389543\n",
      "  0.07017438  0.11392003  0.088386    0.26354736  0.21816969  0.16492513\n",
      "  0.19702578  0.16290769  0.20202614  0.12121037  0.0841052   0.26837045\n",
      "  0.18999547  0.14680132  0.14692144  0.19372413  0.15669532  0.19205335\n",
      "  0.22199905  0.09749731  0.20038038  0.03050791  0.09017105  0.18457381\n",
      "  0.23893625  0.14295755  0.27297196  0.01802626  0.21029136  0.19785121\n",
      "  0.13352394  0.18289354  0.15357909  0.21699323  0.11646929  0.16568027\n",
      "  0.19771424  0.25690958  0.2229301   0.17803489  0.05676816  0.1638026\n",
      "  0.08747561  0.18293336  0.2002433   0.12416811  0.24098599  0.19682732\n",
      "  0.23144358  0.1819497   0.1059739   0.21606538  0.2006805   0.17389232\n",
      "  0.08784609  0.20611686  0.02439824  0.20971459  0.15089284  0.22468275\n",
      "  0.21535178  0.10698203  0.18034026  0.12650073  0.20889953  0.18978927\n",
      "  0.14623977  0.18451563  0.21236426  0.20581856  0.13166876  0.141355\n",
      "  0.19011788  0.20532142  0.22703005  0.15150973  0.16328827  0.12703294\n",
      "  0.2582009   0.16409527  0.07721725  0.2503937   0.16783069  0.21846095\n",
      "  0.16275328  0.13464306  0.13340631  0.14150763  0.17197867  0.14567402\n",
      "  0.1531733   0.1531733   0.12795898  0.12795898  0.17869641  0.17869641\n",
      "  0.15710586  0.15710586  0.18717161  0.18717161  0.2769488   0.2769488\n",
      "  0.1597003   0.1597003   0.10486948  0.10486948  0.21959178  0.21959178\n",
      "  0.15637465  0.15637465  0.09649865  0.09649865  0.18844645  0.18844645\n",
      "  0.18096112  0.18096112  0.13047114  0.13047114 -0.07235724 -0.07235724\n",
      "  0.13110422  0.13110422  0.1014601   0.1014601   0.02752169  0.02752169]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "anies have embraced a culture of opacity about their most cutting edge research stateofai  16 The GPT4 technical report puts the nail in the cofÔ¨Ån of SOTA LLM research Introduction  Research  Industry  Politics  Safety  Predictions   "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "er training on the users speciÔ¨Åc use case But thats hindered by a limited context length due to the resulting compute and memory bottleneck  Several innovations have been used to increase the context length of LLMs Some fundamentally make the memory footprint of attention smaller FlashAttention Others enable models to train on small contexts but run inference on larger ones ALiBi  this is called length extrapolation  at the price of minimal Ô¨Ånetuning and removing positional e"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "stateofai 2023  FlashAttention introduces a signiÔ¨Åcant memory saving by making attention linear instead of quadratic in sequence length FlashAttention2 further improves computing the attention matrix by having fewer nonmatmul FLOPS better parallelism and better work partitioning The result is a 28x training speedup of GPTstyle models  Reducing the number of bits in the parameters reduces both the memory footprint and the latency of LLMs The case for 4bit precision kbit Inferenc"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "models in popular products most notably on Adobes FireÔ¨Çy Photoroom or even Discord stateofai  93 Texttoimage models Competition intensiÔ¨Åes and integrations abound Introduction  Research  Industry  Politics  Safety  Predictions  Midjourneys revenue which had already reached 1M MRR in March 2022 is projected to reach 200M ARR in 2023 Its number of users grew from 2M to 148M YoY Notably Midjourney is integrated in Discord where users can generate images on a Discord s"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "greement with Google DeepMind Anthropic and OpenAI to gain early access to their most advanced frontier models to improve their understanding of risk  While popular with industry it is unclear if these approaches will survive Recently the UK Government dropped lighttouch from its vocabulary and has repositioned itself as the home of the AI safety debate  The Indian Ministry of Electronics and Information Technology has now said forthcoming legislation may indeed cover some forms of"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from superduperdb import Document\n",
    "from IPython.display import *\n",
    "\n",
    "# Execute a query to find text chunks\n",
    "context = db.execute(\n",
    "      book\n",
    "        .like(Document({'text_chunk': 'What is new about FlashAttention?'}), vector_index='book-index', n=5)\n",
    "        .limit(5)\n",
    ")\n",
    "\n",
    "context_str = \"\"\n",
    "# Display a horizontal rule to separate results\n",
    "display(Markdown('---'))\n",
    "\n",
    "# Display each document's 'txt' field and separate them with a horizontal rule\n",
    "for r in context:\n",
    "    display(Markdown(r['text_chunk']))\n",
    "    display(Markdown('---'))\n",
    "# context[0]['text_chunk']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b75122",
   "metadata": {},
   "source": [
    "# Step 3: DeciLM 7B & SuperDuperDB again to do LLM-powered Chat Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c01778-8722-41d9-8a31-bb0e71fa8695",
   "metadata": {},
   "source": [
    "## DeciLM-7B-instruct\n",
    "\n",
    "Now it's inferencing time. We picked the DeciLM 7B Instruct for this job. As it is promising highest possible throughput right now. \n",
    "\n",
    "Here you can use any other model. It's just basic Huggingface stuffs! Find your model and do your thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e91d50d5-2482-40d9-9652-194d781e474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df86d37e-f4ad-4de6-b0fd-983ca1004b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'Deci/DeciLM-7B-instruct'\n",
    "\n",
    "using_colab_T4_GPU = False # We used A10 GPU, if you have T4 make it true\n",
    "if using_colab_T4_GPU:\n",
    "  bnb_config = BitsAndBytesConfig(\n",
    "      load_in_4bit = True,\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16\n",
    "  )\n",
    "  dtype_kwargs = {\"quantization_config\": bnb_config}\n",
    "else:\n",
    "  dtype_kwargs = {\"torch_dtype\": torch.bfloat16}\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    **dtype_kwargs\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token # For DeciLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be124fb8",
   "metadata": {},
   "source": [
    "Here is a helper function to generate prompt with DeciLM 7B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9017c66-ef0b-4ab3-9a5a-83d6c5f3726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_TEMPLATE =\"\"\"\n",
    "### System:\n",
    "You are an AI assistant that follows instruction extremely well. Help as much as you can.\n",
    "### User:\n",
    "{instruction}\n",
    "### Assistant:\n",
    "\"\"\"\n",
    "\n",
    "# Function to construct the prompt using the new system prompt template\n",
    "def get_prompt_with_template(message: str) -> str:\n",
    "    return SYSTEM_PROMPT_TEMPLATE.format(instruction=message)\n",
    "\n",
    "# Function to handle the generation of the model's response using the constructed prompt\n",
    "def generate_model_response(message: str) -> str:\n",
    "    prompt = get_prompt_with_template(message)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    if torch.cuda.is_available():  # Ensure input tensors are on the GPU if model is on GPU\n",
    "        inputs = inputs.to('cuda')\n",
    "    output = model.generate(**inputs,\n",
    "                            max_new_tokens=3000,\n",
    "                            num_beams=5,\n",
    "                            no_repeat_ngram_size=4,\n",
    "                            early_stopping=True\n",
    "                            )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Function to extract the content after \"### Response:\"\n",
    "def extract_response_content(full_response: str) -> str:\n",
    "    response_start_index = full_response.find(\"### Assistant:\")\n",
    "    if response_start_index != -1:\n",
    "        return full_response[response_start_index + len(\"### Assistant:\"):].strip()\n",
    "    else:\n",
    "        return full_response\n",
    "\n",
    "# Main function to get the model's response and extract the content after \"### Response:\"\n",
    "def get_response_with_template(message: str) -> str:\n",
    "    full_response = generate_model_response(message)\n",
    "    return extract_response_content(full_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c7059",
   "metadata": {},
   "source": [
    "Now create a helper function to do `vector-search` and get the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e7311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get the context text\n",
    "\n",
    "from superduperdb import Document\n",
    "\n",
    "def get_context(query, number = 1):\n",
    "    # Execute a query to find text chunks\n",
    "    contexts = db.execute(\n",
    "          book\n",
    "            .like(Document({'text_chunk': 'query'}), vector_index='book-index', n=number)\n",
    "            .limit(number)\n",
    "    )\n",
    "\n",
    "    context_str = []\n",
    "\n",
    "    # Display each document's 'txt' field and separate them with a horizontal rule\n",
    "    for context in contexts:\n",
    "        context_str.append(context['text_chunk'])\n",
    "\n",
    "    result = ' '.join(context_str)\n",
    "\n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0021b1f9",
   "metadata": {},
   "source": [
    "Now start chatting with your LLM based on your LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4496aa03-3b21-4e7e-ba7e-f6954b8ee27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 168.23it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2023-Dec-15 13:22:06.51\u001b[0m| \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mip-172-31-29-75\u001b[0m| \u001b[36ma7d1f8ff-42be-476b-9adc-74c4b826ac32\u001b[0m| \u001b[36msuperduperdb.vector_search.in_memory\u001b[0m:\u001b[36m67  \u001b[0m | \u001b[34m\u001b[1m[ 4.51766104e-02  3.04632280e-02  4.90104482e-02  6.72234818e-02\n",
      "  2.07076222e-03  5.07198870e-02 -5.88827953e-03  7.12032989e-03\n",
      " -2.18982063e-02  5.78837246e-02 -3.89772989e-02  1.20863691e-03\n",
      "  4.93805856e-05 -7.13142008e-02 -2.44551897e-03  1.21803656e-02\n",
      "  7.72564113e-02  2.86947656e-02  5.74232340e-02  7.13339224e-02\n",
      "  5.26450053e-02  2.86853388e-02 -3.29149067e-02  1.61617026e-02\n",
      " -3.23099345e-02 -1.29313581e-02  1.39991343e-01  8.04095156e-03\n",
      " -2.44408958e-02  2.88190488e-02  7.81827420e-03 -1.08232601e-02\n",
      " -4.90357503e-02 -1.92871504e-02  9.33369398e-02  8.08862671e-02\n",
      "  9.14772004e-02  7.28338063e-02  3.91569398e-02  1.40876174e-02\n",
      " -7.79762911e-03 -2.20977813e-02  5.46424799e-02  4.43230122e-02\n",
      "  2.14818865e-03  2.06977986e-02 -5.94393536e-03  5.32741621e-02\n",
      " -3.14332359e-02 -6.79176301e-05  1.06060766e-01 -4.30314802e-02\n",
      "  2.56059840e-02 -2.83036400e-02 -2.91504897e-03  1.30664125e-01\n",
      " -4.30003777e-02 -6.98280428e-03  3.41374762e-02 -2.12250277e-03\n",
      " -2.70050541e-02 -4.55609411e-02 -5.16907871e-02  8.10640305e-02\n",
      "  3.27043682e-02 -2.42667161e-02  4.23212629e-03 -2.30196081e-02\n",
      " -1.46560550e-01  3.50247100e-02  2.80929394e-02  4.92882282e-02\n",
      "  5.81923984e-02  3.29124629e-02  1.17172543e-02  1.70661896e-01\n",
      " -7.58806756e-03  1.22278705e-01  1.19224265e-02  4.42452058e-02\n",
      " -3.21347266e-02  2.74929497e-02  4.83113043e-02 -3.19235548e-02\n",
      "  1.77123807e-02 -8.33165944e-02  6.22537583e-02 -3.15249190e-02\n",
      " -2.32269093e-02 -2.19811536e-02  4.54588234e-02 -7.64421187e-03\n",
      " -2.59235278e-02  1.27584264e-01  3.05652656e-02  1.16950981e-02\n",
      "  2.57216096e-02  1.21639147e-02 -5.40447645e-02  2.15277635e-03\n",
      " -3.92395854e-02  3.75772342e-02  5.53692356e-02  1.56849790e-02\n",
      "  3.17435190e-02 -6.69585355e-03  7.64872599e-03  1.14306249e-01\n",
      "  2.37440206e-02  5.19660972e-02 -1.13887433e-02 -1.22880600e-02\n",
      " -3.87945622e-02 -6.35946244e-02  7.98576102e-02  6.99156374e-02\n",
      " -1.13638043e-02  9.92981493e-02 -3.79174426e-02  2.06909291e-02\n",
      " -5.44149801e-03  1.12471268e-01 -1.09646767e-02  5.59152588e-02\n",
      " -2.86476966e-02  3.11018899e-02  2.47254781e-02  8.98320228e-02\n",
      "  1.09103151e-01  1.08599272e-02  5.96094765e-02  1.05946735e-01\n",
      "  3.23513430e-03 -6.09501675e-02  5.75302057e-02 -3.54445577e-02\n",
      "  1.27136819e-02  5.30977473e-02  1.45432875e-02  2.58783884e-02\n",
      " -1.33781526e-02  4.98036630e-02 -3.66652198e-03 -6.58449344e-03\n",
      "  1.82580218e-01  5.48871756e-02 -2.14236267e-02 -7.13402927e-02\n",
      "  2.29929555e-02 -2.69042347e-02 -7.88362995e-02 -1.08058397e-02\n",
      "  1.92662433e-01  1.52669832e-01  1.70661896e-01  2.45036073e-02\n",
      " -2.86630839e-02  5.68993390e-02  2.47584470e-02 -7.47601464e-02\n",
      " -2.68610734e-02  9.29989070e-02  3.92663814e-02 -1.49713773e-02\n",
      "  2.25098561e-02  9.84107852e-02  6.22274280e-02  2.38895901e-02\n",
      " -2.84587145e-02  4.56872359e-02 -2.19404697e-04  4.12517376e-02\n",
      "  4.98221666e-02 -5.14643714e-02  3.16845104e-02  4.71760035e-02\n",
      " -2.85570752e-02  7.74595886e-04 -2.82243211e-02 -9.34150070e-03\n",
      "  1.05441920e-01 -2.13850662e-02  1.73893496e-02 -1.33796446e-02\n",
      "  2.28438750e-02  4.53547686e-02 -8.71855952e-03  1.23886745e-02\n",
      " -1.23777958e-02 -1.14784166e-02  5.05065769e-02  1.75540894e-02\n",
      "  1.13180697e-01  1.61655873e-01 -7.42242187e-02 -4.05062810e-02\n",
      " -4.55067046e-02 -1.72664896e-02  2.75213644e-02  5.28504699e-02\n",
      " -1.99155454e-02 -1.19743785e-02 -3.27082500e-02 -3.68499979e-02\n",
      " -4.02550995e-02 -9.64583829e-04  3.28413434e-02  1.34414528e-03\n",
      "  9.24991444e-03 -8.20854679e-03  1.15524895e-01  2.94742230e-02\n",
      "  4.34863493e-02  4.73014824e-02  3.12248152e-02 -4.21870947e-02\n",
      "  7.02000335e-02 -3.15732509e-03  8.32044929e-02  6.09013774e-02\n",
      " -3.24507728e-02  8.64049271e-02 -1.87135041e-02  8.86614621e-03\n",
      " -5.79775572e-02 -3.07900384e-02  7.09843934e-02 -7.90405273e-02\n",
      " -3.25641148e-02  1.18727610e-03  1.61243677e-02 -2.67147608e-02\n",
      " -6.65057898e-02  9.90757346e-02 -5.65215796e-02  6.81761950e-02\n",
      "  5.02537861e-02  1.04397349e-02  1.21898036e-02 -6.29404262e-02\n",
      "  3.41027901e-02  5.32843322e-02  1.31484747e-01 -2.33700871e-02\n",
      "  5.41439950e-02  4.65457216e-02  1.41734242e-01 -1.05671398e-02\n",
      " -2.11792327e-02  9.54230726e-02 -1.27937533e-02 -3.04454491e-02\n",
      "  6.81410357e-02  1.33473054e-03  6.00739010e-02  1.78781748e-02\n",
      " -3.84774525e-03  4.99853492e-02 -2.83608772e-02 -4.32481468e-02\n",
      " -3.04920599e-03  5.37215360e-02 -8.97217691e-02  2.09071599e-02\n",
      "  3.94163020e-02 -6.04999810e-02  6.16229959e-02 -7.60223530e-03\n",
      "  8.79595987e-03  8.32237005e-02  3.26243937e-02 -1.17963050e-02\n",
      "  3.97360250e-02  1.03602432e-01 -9.04565528e-02 -6.10506628e-03\n",
      "  6.16243780e-02  2.03289464e-03  9.48481336e-02 -2.67967656e-02\n",
      " -7.50483945e-03  2.33647935e-02 -2.25523673e-03  2.48884782e-03\n",
      "  4.15141806e-02  1.02467373e-01 -1.89449824e-02 -3.59613933e-02\n",
      " -1.57305505e-02 -1.65485609e-02 -1.07048973e-02  3.28005329e-02\n",
      " -2.86743231e-02  7.16412067e-03 -3.45127843e-03  1.64762195e-02\n",
      "  1.53487045e-02  1.45401582e-02  5.19262254e-02 -1.09545439e-02\n",
      "  1.58631712e-01 -1.77325979e-02 -5.78925423e-02 -4.12313752e-02\n",
      "  3.93677577e-02  5.34824282e-02  8.37122202e-02 -1.56186912e-02\n",
      "  3.48089188e-02  6.91547431e-03  1.96685240e-01 -9.34652761e-02\n",
      "  9.10166800e-02 -5.16308583e-02  1.69529356e-02  3.42439935e-02\n",
      " -3.04527413e-02  1.58411674e-02  2.80626304e-02  5.14734313e-02\n",
      " -6.61484748e-02  1.12953102e-02  4.16861475e-02  3.96440439e-02\n",
      "  5.91742881e-02  3.80880199e-04 -2.72541866e-02 -1.35383159e-02\n",
      " -1.62296947e-02  4.15885970e-02  6.55040741e-02  2.40220819e-02\n",
      "  3.74400169e-02 -1.18990839e-02 -4.24637757e-02  8.46148878e-02\n",
      " -1.31143546e-02  1.57762706e-01 -7.99105503e-04  6.76221550e-02\n",
      "  2.79741734e-02 -2.52047013e-02 -1.37834959e-02  4.61162664e-02\n",
      " -5.40424995e-02  1.78170912e-02  5.18070534e-04  8.78646970e-02\n",
      "  5.61429784e-02 -2.89653745e-02  2.66370773e-02 -6.22304082e-02\n",
      "  3.93762626e-03  5.04118390e-03  2.88931504e-02  5.88478819e-02\n",
      "  9.62838456e-02  1.17724068e-01  2.75428034e-02  9.05136913e-02\n",
      " -2.08165497e-03  9.76019800e-02  4.15384509e-02  2.35263035e-02\n",
      " -4.02550995e-02  1.47691313e-02 -2.61830613e-02 -2.61830613e-02\n",
      " -1.06390193e-02 -1.06390193e-02  3.78638096e-02  3.78638096e-02\n",
      " -1.76233649e-02 -1.76233649e-02 -5.30426204e-03 -5.30426204e-03\n",
      "  6.82796165e-03  6.82796165e-03  8.10773298e-03  8.10773298e-03\n",
      "  4.55155596e-02  4.55155596e-02  4.38651182e-02  4.38651182e-02\n",
      "  3.89496610e-02  3.89496610e-02  1.26488999e-01  1.26488999e-01\n",
      "  8.48110020e-03  8.48110020e-03  8.70181844e-02  8.70181844e-02\n",
      "  9.82561409e-02  9.82561409e-02  1.07797891e-01  1.07797891e-01\n",
      "  9.20541286e-02  9.20541286e-02  5.17022386e-02  5.17022386e-02\n",
      "  1.31484762e-01  1.31484762e-01]\u001b[0m\n",
      "Based on the provided context, the new aspect of FlashAttention is the integration of Chain of Thought (CoT) and Tree of Thought (ToT) prompting techniques. These techniques aim to improve the quality of prompts and enhance task performance by incorporating intermediate reasoning steps and representing thoughts as a tree structure, respectively. Additionally, FlashAttention leverages search algorithms to explore the tree structure and assigns probabilities to answer binary questions.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is new about FlashAttention?\"\n",
    "context = get_context(query, 5)\n",
    "\n",
    "prompt = f\"Your task is to synthesize the query, which is delimited by triple backticks, and write a response that appropriately answers the query based on the retrieved context.\\n### Query:\\n```{query}```\\n### Context:\\n```{context}```\\n### Response:\\nBegin!\"\n",
    "\n",
    "# Sample usage\n",
    "# user_message = f\"{query}. Context: {context}\"\n",
    "response = get_response_with_template(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ab8fb",
   "metadata": {},
   "source": [
    "Result: \n",
    "Based on the provided context, the new aspect of FlashAttention is the integration of Chain of Thought (CoT) and Tree of Thought (ToT) prompting techniques. These techniques aim to improve the quality of prompts and enhance task performance by incorporating intermediate reasoning steps and representing thoughts as a tree structure, respectively. Additionally, FlashAttention leverages search algorithms to explore the tree structure and assigns probabilities to answer binary questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791c3697",
   "metadata": {},
   "source": [
    "# Bring you own LLM from HuggingFace or anywhere! \n",
    "In step 3 you can bring any model you like from Huggingface ecosystem. SuperDuperDB is here to help to generate the context for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89cebd4",
   "metadata": {},
   "source": [
    "## You see the power of SuperDuperDB and how it blends well with the ecosystem.\n",
    "\n",
    "You see SuperDuperDB is helping you to generate context for your LLM. Now you can use any other LLM. Just edit the f-string of prompt with the context produced by SuperDuperDB!\n",
    "\n",
    "Now create your own solution and share it to us. Maybe create a database with 100 pdfs! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
